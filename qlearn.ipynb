{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kVd1ovCggTd"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "env = gym.make(\"Boxing-ram-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObBpwav6gsLb"
   },
   "outputs": [],
   "source": [
    "def build_state(features):\n",
    "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuXjDGxYgwi5"
   },
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha  # discount constant\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "        '''\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q);\n",
    "            mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "            # return random.choice(self.actions)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values\n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "        action = self.actions[i]\n",
    "        if return_q:  # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma * maxqnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRZLjCd1luH4"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_log(episode, g):\n",
    "    f = open(f'./log.csv', mode='a+')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([episode, g])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ac8PoSZXg8AX"
   },
   "outputs": [],
   "source": [
    "def learn_one_episode(Q, episode):\n",
    "  done = False\n",
    "  G, reward = 0, 0\n",
    "  state = env.reset()\n",
    "  while not done:\n",
    "      action = Q.chooseAction(build_state(state))\n",
    "      state2, reward, done, info = env.step(action)\n",
    "      Q.learn(build_state(state), action, reward, build_state(state))\n",
    "      G += reward\n",
    "      state = state2\n",
    "  csv_log(episode, G)\n",
    "  print('Episode {} Total Reward: {}'.format(episode, G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "94qsRoYyg2Ao",
    "outputId": "0878e85e-7801-47d6-d13d-fcfa57ea19c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Total Reward: -2.0\n",
      "Episode 2 Total Reward: 1.0\n",
      "Episode 3 Total Reward: 10.0\n",
      "Episode 4 Total Reward: 10.0\n",
      "Episode 5 Total Reward: -3.0\n",
      "Episode 6 Total Reward: 13.0\n",
      "Episode 7 Total Reward: 6.0\n",
      "Episode 8 Total Reward: 4.0\n",
      "Episode 9 Total Reward: 1.0\n",
      "Episode 10 Total Reward: -2.0\n",
      "Episode 11 Total Reward: -5.0\n",
      "Episode 12 Total Reward: 1.0\n",
      "Episode 13 Total Reward: 5.0\n",
      "Episode 14 Total Reward: -2.0\n",
      "Episode 15 Total Reward: -2.0\n",
      "Episode 16 Total Reward: 1.0\n",
      "Episode 17 Total Reward: 2.0\n",
      "Episode 18 Total Reward: 1.0\n",
      "Episode 19 Total Reward: 1.0\n",
      "Episode 20 Total Reward: 0.0\n",
      "Episode 21 Total Reward: -1.0\n",
      "Episode 22 Total Reward: 4.0\n",
      "Episode 23 Total Reward: -2.0\n",
      "Episode 24 Total Reward: 1.0\n",
      "Episode 25 Total Reward: 1.0\n",
      "Episode 26 Total Reward: 4.0\n",
      "Episode 27 Total Reward: -5.0\n",
      "Episode 28 Total Reward: 5.0\n",
      "Episode 29 Total Reward: -9.0\n",
      "Episode 30 Total Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "Q = QLearn(list(range(0, 18)), 0.4, 0.618, 0.9)\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)\n",
    "\n",
    "Q.epsilon = 0.3\n",
    "Q.alpha = 0.518\n",
    "Q.gamma = 0.8\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)\n",
    "\n",
    "\n",
    "Q.epsilon = 0.2\n",
    "Q.alpha = 0.418\n",
    "Q.gamma = 0.7\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)\n",
    "\n",
    "Q.epsilon = 0.1\n",
    "Q.alpha = 0.318\n",
    "Q.gamma = 0.6\n",
    "\n",
    "for episode in range(1, 1001):\n",
    "  learn_one_episode(Q, episode)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "qlearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
